{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from torch import optim\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import cvxpy as cp\n",
    "import mosek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2021\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lpsolver(prob, verbose=False):\n",
    "#   print('=== LP Solver ===')\n",
    "  solvers = [cp.MOSEK, cp.ECOS_BB]\n",
    "  for s in solvers:\n",
    "    # print('==> Invoking {}...'.format(s))\n",
    "    try:\n",
    "      result = prob.solve(solver=s, verbose=verbose)\n",
    "      return result\n",
    "    except cp.error.SolverError as e:\n",
    "      print('==> Solver Error')\n",
    "\n",
    "#   print('==> Invoking MOSEK simplex method...')\n",
    "  try:\n",
    "    result = prob.solve(solver=cp.MOSEK,\n",
    "                      mosek_params={mosek.iparam.optimizer: mosek.optimizertype.free_simplex},\n",
    "                      bfs=True, verbose=verbose)\n",
    "    return result\n",
    "  except cp.error.SolverError as e:\n",
    "    print('==> Solver Error')\n",
    "\n",
    "  raise cp.error.SolverError('All solvers failed.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    super(MyDataset, self).__init__()\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.attr = X\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    return self.X[item], self.y[item]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "def gen_data(n):\n",
    "      Y = np.random.choice([0, 1], size=n, p=[0.5, 0.5])\n",
    "      X = np.zeros((n, 2), dtype=np.float32)\n",
    "      # Gaussian\n",
    "      X[Y == 0] = np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], size=(Y == 0).sum())\n",
    "      # mixture of two Gaussians\n",
    "      mask = Y == 1\n",
    "      # subsample mask with 50% probability\n",
    "      mask_g1 = mask & (np.random.rand(n) < 0.2)\n",
    "      mask_g2 = mask & ~mask_g1\n",
    "      X[mask_g1] = np.random.multivariate_normal([1, 3], [[1, 0], [0, 1]], size=mask_g1.sum())\n",
    "      X[mask_g2] = np.random.multivariate_normal([3, 0], [[1, 0], [0, 1]], size=mask_g2.sum())\n",
    "      return X, Y\n",
    "\n",
    "X_train, y_train = gen_data(1000)\n",
    "X_test, y_test = gen_data(1000)\n",
    "# print(X_train.shape, y_train.shape)\n",
    "dataset_train = MyDataset(X_train, y_train)\n",
    "dataset_test_train = MyDataset(X_train, y_train)\n",
    "dataset_valid = MyDataset(X_test, y_test)\n",
    "dataset_test = MyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], label='class 0', s=5)\n",
    "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], label='class 1', s=5)\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(2, 2, bias=True))\n",
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erm(model: Module, loader: DataLoader, optimizer: optim.Optimizer, criterion, device: str, iters=0):\n",
    "  \"\"\"Empirical Risk Minimization (ERM)\"\"\"\n",
    "\n",
    "  model.train()\n",
    "  iteri = 0\n",
    "  for _, (inputs, targets) in enumerate(loader):\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets).mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    iteri += 1\n",
    "    if iteri == iters:\n",
    "      break\n",
    "\n",
    "def test(model: Module, loader: DataLoader, criterion, device: str):\n",
    "  \"\"\"Test the avg and group acc of the model\"\"\"\n",
    "\n",
    "  model.eval()\n",
    "  total_correct = 0\n",
    "  total_loss = 0\n",
    "  total_num = 0\n",
    "  l_rec = []\n",
    "  c_rec = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for _, (inputs, targets) in enumerate(loader):\n",
    "      inputs, targets = inputs.to(device), targets.to(device)\n",
    "      labels = targets\n",
    "      outputs = model(inputs)\n",
    "      predictions = torch.argmax(outputs, dim=1)\n",
    "      c = (predictions == labels)\n",
    "      c_rec.append(c.detach().cpu().numpy())\n",
    "      correct = c.sum().item()\n",
    "      l = criterion(outputs, labels).view(-1)\n",
    "      l_rec.append(l.detach().cpu().numpy())\n",
    "      loss = l.sum().item()\n",
    "      total_correct += correct\n",
    "      total_loss += loss\n",
    "      total_num += len(inputs)\n",
    "  l_vec = np.concatenate(l_rec)\n",
    "  c_vec = np.concatenate(c_rec)\n",
    "  return total_correct / total_num, total_loss / total_num, c_vec, l_vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAI GAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost(l_all, w_all):\n",
    "  acc = 1 - l_all[-1, :]\n",
    "  lss = 1 - acc\n",
    "  weight = w_all[-1, :]\n",
    "  eps = lss.T @ weight\n",
    "  beta = eps / (1 - eps)\n",
    "  weight *= np.power(beta, lss)\n",
    "  weight /= weight.sum()\n",
    "  return weight, 0\n",
    "\n",
    "def raigame(P, l_all, eta, constraints_w=['chi2'], alpha=0.99):\n",
    "    num_epochs, n = l_all.shape\n",
    "    w = cp.Variable(n)\n",
    "    \n",
    "    objective = cp.Maximize(P @ (l_all @ w) + eta * cp.sum(cp.entr(w)))\n",
    "    constraints = []\n",
    "    constraints.append(cp.sum(w) == 1)\n",
    "    constraints.append(0 <= w)\n",
    "    # add constraints\n",
    "    nc = len(constraints_w)\n",
    "    if 'cvar' in constraints_w:\n",
    "      m = alpha * n\n",
    "      constraints.append(w <= 1 / m)\n",
    "      nc -= 1\n",
    "    if 'chi2' in constraints_w:\n",
    "      m = alpha * n\n",
    "      constraints.append(cp.sum_squares(w) <= 1 / m)\n",
    "      nc -= 1\n",
    "    if nc > 0:\n",
    "      print('Constraint {} not implemented'.format(constraints_w))\n",
    "      raise NotImplementedError\n",
    "    #################\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    result = lpsolver(prob, verbose=False)\n",
    "    \n",
    "    w_star = w.value\n",
    "    w_star[w_star < 0] = 0\n",
    "    w_star /= w_star.sum()\n",
    "    return w_star, result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 10\n",
    "iters_per_epoch = 1000\n",
    "batch_size = 32\n",
    "w_all = np.zeros((0, len(dataset_train)))\n",
    "l_all = np.zeros((0, len(dataset_train)))\n",
    "P = np.zeros((1, 0))\n",
    "n = len(dataset_train)\n",
    "eta = (np.log(T) / (2 * T * np.log(n))) ** 0.5\n",
    "lamda = -1/2\n",
    "print(eta, eta * np.log(n))\n",
    "\n",
    "init_state_dict = deepcopy(model.state_dict())\n",
    "w_all = np.concatenate([w_all, np.ones((1, n)) / n])\n",
    "\n",
    "test_trainloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False) # for testing\n",
    "prev_gamevalue = np.inf\n",
    "best_gamevalue, best_P = np.inf, None\n",
    "models = []\n",
    "for t in range(T):\n",
    "    #set-up model\n",
    "    model.load_state_dict(init_state_dict)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    #set-up loader\n",
    "    w_t = w_all[-1, :]\n",
    "    sampler = WeightedRandomSampler(w_t, iters_per_epoch * batch_size, replacement=True)\n",
    "    trainloader = DataLoader(dataset_train, batch_size=batch_size, sampler=sampler, num_workers=0, pin_memory=False)\n",
    "    \n",
    "    #get ht (or lt)    \n",
    "    erm(model, trainloader, optimizer, loss, 'cpu', iters_per_epoch)\n",
    "    models.append(deepcopy(model.state_dict()))\n",
    "    _, _, c_t, ce_t = test(model, test_trainloader, loss, 'cpu')\n",
    "    l_t = 1 - c_t # get 0-1 loss\n",
    "    l_all = np.concatenate([l_all, l_t.reshape(1, -1)], axis=0)\n",
    "    print('t={}, l_t={:.4f}, ce_t={:.4f}'.format(t, l_t.mean(), ce_t.mean()))\n",
    "    \n",
    "    # do line search for a    \n",
    "    # base_a = 1 / (t + 1)\n",
    "    base_a = 1\n",
    "    best_value = np.inf\n",
    "    best_a = None\n",
    "    for m_a in np.linspace(0.01, 1.01, 20):\n",
    "        a = base_a * m_a\n",
    "        # if a >= 1:\n",
    "        #     continue\n",
    "        P_temp = np.concatenate([P, a * np.ones((1, 1))], axis=1)\n",
    "        P_temp = P_temp / P_temp.sum()\n",
    "        _, value = raigame(P_temp, l_all + lamda, eta)\n",
    "        assert value < np.inf\n",
    "        if value < best_value:\n",
    "            best_value = value\n",
    "            best_a = a\n",
    "    P = np.concatenate([P, best_a * np.ones((1, 1))], axis=1)\n",
    "    print('best_a = {}, best_value = {}'.format(best_a, best_value))\n",
    "    # P = P / P.sum()\n",
    "    \n",
    "    # get new game value\n",
    "    _, gamevalue = raigame(P / P.sum(), l_all, 0)\n",
    "    print('t = {}, gamevalue = {}, max_w_star = {}'.format(t + 1, gamevalue, w_t.max()))\n",
    "    \n",
    "    # update best gamevalue\n",
    "    if gamevalue < best_gamevalue:\n",
    "        best_gamevalue = gamevalue\n",
    "        best_P = P\n",
    "        \n",
    "    # update eta if gamevalue increases\n",
    "    if prev_gamevalue < gamevalue:\n",
    "        eta = eta * 2\n",
    "    prev_gamevalue = gamevalue\n",
    "    \n",
    "    #get wt\n",
    "    w_t, L_P = raigame(P, l_all, eta)\n",
    "    # w_t, _ = adaboost(l_all, w_all)   \n",
    "    w_all = np.concatenate([w_all, w_t.reshape(1, -1)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = l_all.mean(axis=1)\n",
    "np.log((1-l) / l) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_P = np.concatenate([best_P, np.zeros((1, T - best_P.shape[1]))], axis=1)\n",
    "P = best_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = P / P.sum()\n",
    "P"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_erm = np.zeros((1, T))\n",
    "P_erm[:, 0] = 1\n",
    "\n",
    "_, game_erm = raigame(P_erm, l_all, 0)\n",
    "_, game_opt = raigame(P, l_all, 0)\n",
    "\n",
    "print('game_erm = {}, game_opt = {}'.format(game_erm, game_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze CVaR Loss\n",
    "def get_cvar_loss(P, l, alpha):\n",
    "    exp_loss = (P @ l).reshape(-1)\n",
    "    exp_loss = np.sort(exp_loss)[::-1]\n",
    "    n = exp_loss.shape[0]\n",
    "    return format(exp_loss[:int(alpha * n)].mean(), '.3f')\n",
    "\n",
    "for alpha in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]:\n",
    "    print('alpha = {}, opt_cvar = {}, erm_cvar = {}'.format(alpha, get_cvar_loss(P, l_all, alpha), get_cvar_loss(P_erm, l_all, alpha)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze worst-class loss\n",
    "def get_worst_class_loss(P, l, dataset):\n",
    "    assert len(dataset) == l.shape[1]\n",
    "    exp_loss = (P @ l).reshape(-1)\n",
    "    num_classes = len(np.unique(dataset.y))\n",
    "    dct = {}\n",
    "    for i in range(num_classes):\n",
    "        idx = np.where(dataset.y == i)[0]\n",
    "        dct[i] = exp_loss[idx].mean()\n",
    "    print(\"Class Loss: \", dct)\n",
    "    print(\"Worst Class Loss: \", max(dct.values()))\n",
    "    return \n",
    "\n",
    "get_worst_class_loss(P, l_all, dataset_train)\n",
    "get_worst_class_loss(P_erm, l_all, dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
