{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from torch import optim\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2021\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lpsolver(prob, verbose=False):\n",
    "#   print('=== LP Solver ===')\n",
    "  solvers = [cp.MOSEK, cp.ECOS_BB]\n",
    "  for s in solvers:\n",
    "    # print('==> Invoking {}...'.format(s))\n",
    "    try:\n",
    "      result = prob.solve(solver=s, verbose=verbose)\n",
    "      return result\n",
    "    except cp.error.SolverError as e:\n",
    "      print('==> Solver Error')\n",
    "\n",
    "#   print('==> Invoking MOSEK simplex method...')\n",
    "  try:\n",
    "    result = prob.solve(solver=cp.MOSEK,\n",
    "                      mosek_params={mosek.iparam.optimizer: mosek.optimizertype.free_simplex},\n",
    "                      bfs=True, verbose=verbose)\n",
    "    return result\n",
    "  except cp.error.SolverError as e:\n",
    "    print('==> Solver Error')\n",
    "\n",
    "  raise cp.error.SolverError('All solvers failed.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    super(MyDataset, self).__init__()\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.attr = X\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    return self.X[item], self.y[item]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "def gen_data(n):\n",
    "      X = np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], size=n)\n",
    "      thr = 2 **0.5\n",
    "      norm = np.linalg.norm(X, axis=1)\n",
    "      to_rem = (norm < 1.3 * thr) & (norm > thr / 1.3)\n",
    "      # subsample\n",
    "      to_rem = to_rem & (np.random.rand(n) < 0.5)\n",
    "      # remove those in the middle\n",
    "      X = X[~to_rem]\n",
    "      Y = np.linalg.norm(X, axis=1) < thr\n",
    "      pop_masks = []\n",
    "      return X.astype(np.float32), Y.astype(np.long), pop_masks\n",
    "\n",
    "X_train, y_train, pop_masks_train = gen_data(5000)\n",
    "X_test, y_test, pop_masks_test = gen_data(5000)\n",
    "# print(X_train.shape, y_train.shape)\n",
    "dataset_train = MyDataset(X_train, y_train)\n",
    "dataset_test_train = MyDataset(X_train, y_train)\n",
    "dataset_valid = MyDataset(X_test, y_test)\n",
    "dataset_test = MyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], label='class 0', s=5)\n",
    "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], label='class 1', s=5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop_masks_train[0].sum(), pop_masks_train[1].sum(), pop_masks_train[2].sum(), pop_masks_train[3].sum(), pop_masks_train[4].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(nn.Linear(2, 2, bias=True))\n",
    "model = nn.Sequential(nn.Linear(2, 4, bias=True), nn.ReLU(), nn.Linear(4, 2, bias=True))\n",
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(model, inputs, targets, criterion, eps=0.5, iters=1):\n",
    "  ori_inputs = inputs\n",
    "  for iter in range(iters):\n",
    "    inputs.requires_grad = True\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets).mean()\n",
    "    loss.backward()\n",
    "    grad = inputs.grad.detach()\n",
    "    inputs.requires_grad = False\n",
    "    delta = eps * torch.sign(grad)\n",
    "    inputs = torch.clamp(ori_inputs + delta, min=-3, max=3)\n",
    "  return inputs\n",
    "\n",
    "def erm(model: Module, loader: DataLoader, optimizer: optim.Optimizer, criterion, device: str, iters=0):\n",
    "  \"\"\"Empirical Risk Minimization (ERM)\"\"\"\n",
    "\n",
    "  model.train()\n",
    "  iteri = 0\n",
    "  for _, (inputs, targets) in enumerate(loader):\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    # add adversarial noise using pgd attack\n",
    "    inputs = pgd_attack(model, inputs, targets, criterion)\n",
    "    #\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets).mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    iteri += 1\n",
    "    if iteri == iters:\n",
    "      break\n",
    "\n",
    "def test(model: Module, loader: DataLoader, criterion, device: str):\n",
    "  \"\"\"Test the avg and group acc of the model\"\"\"\n",
    "\n",
    "  model.eval()\n",
    "  total_correct = 0\n",
    "  total_loss = 0\n",
    "  total_num = 0\n",
    "  l_rec = []\n",
    "  c_rec = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for _, (inputs, targets) in enumerate(loader):\n",
    "      inputs, targets = inputs.to(device), targets.to(device)\n",
    "      labels = targets\n",
    "      outputs = model(inputs)\n",
    "      predictions = torch.argmax(outputs, dim=1)\n",
    "      c = (predictions == labels)\n",
    "      c_rec.append(c.detach().cpu().numpy())\n",
    "      correct = c.sum().item()\n",
    "      l = criterion(outputs, labels).view(-1)\n",
    "      l_rec.append(l.detach().cpu().numpy())\n",
    "      loss = l.sum().item()\n",
    "      total_correct += correct\n",
    "      total_loss += loss\n",
    "      total_num += len(inputs)\n",
    "  l_vec = np.concatenate(l_rec)\n",
    "  c_vec = np.concatenate(c_rec)\n",
    "  return total_correct / total_num, total_loss / total_num, c_vec, l_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(nn.Linear(2, 4, bias=True), nn.ReLU(), nn.Linear(4, 2, bias=True))\n",
    "\n",
    "# batch_size = 512\n",
    "# iters = 1000\n",
    "\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "# trainloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "# for i in range(50):\n",
    "#     erm(model, trainloader, optimizer, loss, 'cpu', iters=iters)\n",
    "#     acc, _, c_vec, _ = test(model, trainloader, loss, 'cpu')\n",
    "#     print(\"Train Acc: \", acc, end=', ')\n",
    "# acc, _, c_vec, _ = test(model, testloader, loss, 'cpu')\n",
    "# print(\"Test Loss: \", 1 - acc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAI GAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raigame(P, l_all, eta, constraints_w=['chi2'], alpha=0.7, group_idx=pop_masks_train, verbose=False):\n",
    "    num_epochs, n = l_all.shape\n",
    "    w = cp.Variable(n)\n",
    "    \n",
    "    objective = cp.Maximize(P @ (l_all @ w) + eta * cp.sum(cp.entr(w)))\n",
    "    constraints = []\n",
    "    constraints.append(cp.sum(w) == 1)\n",
    "    constraints.append(0 <= w)\n",
    "    # add constraints\n",
    "    nc = len(constraints_w)\n",
    "    if 'cvar' in constraints_w:\n",
    "      m = alpha * n\n",
    "      constraints.append(w <= 1 / m)\n",
    "      nc -= 1\n",
    "    if 'chi2' in constraints_w:\n",
    "      m = alpha * n\n",
    "      constraints.append(cp.sum_squares(w) <= 1 / m)\n",
    "      nc -= 1\n",
    "    if 'group_dro' in constraints_w:\n",
    "      for idx in group_idx:\n",
    "        # get all the indices of the group\n",
    "        locs = np.where(idx)[0]\n",
    "        for i in locs:\n",
    "          constraints.append(w[i] == w[locs[0]])\n",
    "      constraints.append(cp.entr(cp.vstack([cp.sum(w[idx]) for idx in group_idx])) >= 0.1)\n",
    "      nc -= 1\n",
    "    if nc > 0:\n",
    "      print('Constraint {} not implemented'.format(constraints_w))\n",
    "      raise NotImplementedError\n",
    "    #################\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    result = lpsolver(prob, verbose=False)\n",
    "    \n",
    "    w_star = w.value\n",
    "    w_star[w_star < 0] = 0\n",
    "    w_star /= w_star.sum()\n",
    "    return w_star, result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 5\n",
    "iters_per_epoch = 3000\n",
    "batch_size = 32\n",
    "w_all = np.zeros((0, len(dataset_train)))\n",
    "l_all = np.zeros((0, len(dataset_train)))\n",
    "P = np.zeros((1, 0))\n",
    "n = len(dataset_train)\n",
    "eta = (np.log(T) / (2 * T * np.log(n))) ** 0.5\n",
    "print(eta, eta * np.log(n))\n",
    "\n",
    "init_state_dict = deepcopy(model.state_dict())\n",
    "w_all = np.concatenate([w_all, np.ones((1, n)) / n])\n",
    "\n",
    "test_trainloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False) # for testing\n",
    "prev_gamevalue = np.inf\n",
    "best_gamevalue, best_P = np.inf, None\n",
    "models = []\n",
    "for t in range(T):\n",
    "    #set-up model\n",
    "    model.load_state_dict(init_state_dict)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=1e-2)\n",
    "    \n",
    "    #set-up loader\n",
    "    w_t = w_all[-1, :]\n",
    "    sampler = WeightedRandomSampler(w_t, iters_per_epoch * batch_size, replacement=True)\n",
    "    trainloader = DataLoader(dataset_train, batch_size=batch_size, sampler=sampler, num_workers=0, pin_memory=False)\n",
    "    \n",
    "    #get ht (or lt)    \n",
    "    erm(model, trainloader, optimizer, loss, 'cpu', iters_per_epoch)\n",
    "    models.append(deepcopy(model.state_dict()))\n",
    "    _, _, c_t, ce_t = test(model, test_trainloader, loss, 'cpu')\n",
    "    l_t = 1 - c_t # get 0-1 loss\n",
    "    print('t={}, loss={}'.format(t, l_t.mean()))\n",
    "    l_all = np.concatenate([l_all, l_t.reshape(1, -1)], axis=0)\n",
    "\n",
    "    # do line search for a    \n",
    "    base_a = 1 / (t + 1)\n",
    "    best_value = np.inf\n",
    "    best_a =  1 / (t + 1)\n",
    "    for m_a in np.linspace(0.5, 1.5, 20):\n",
    "        a = base_a * m_a\n",
    "        if a >= 1:\n",
    "            continue\n",
    "        P_temp = np.concatenate([(1 - a) * P, a * np.ones((1, 1))], axis=1)\n",
    "        P_temp = P_temp / P_temp.sum()\n",
    "        _, value = raigame(P_temp, l_all, 0)\n",
    "        if not value < np.inf:\n",
    "            continue\n",
    "        if value < best_value:\n",
    "            best_value = value\n",
    "            best_a = a\n",
    "    P = np.concatenate([(1 - best_a) * P, best_a * np.ones((1, 1))], axis=1)\n",
    "    P = P / P.sum()\n",
    "    \n",
    "    # get new game value\n",
    "    _, gamevalue = raigame(P, l_all, 0)\n",
    "    print('t = {}, gamevalue = {}, max_w_star = {}'.format(t + 1, gamevalue, w_t.max()))\n",
    "    \n",
    "    # update best gamevalue\n",
    "    if gamevalue < best_gamevalue:\n",
    "        best_gamevalue = gamevalue\n",
    "        best_P = P\n",
    "        \n",
    "    # update eta if gamevalue increases\n",
    "    if prev_gamevalue < gamevalue:\n",
    "        eta = eta * 2\n",
    "    prev_gamevalue = gamevalue\n",
    "    \n",
    "    #get wt\n",
    "    w_t, L_P = raigame(P, l_all, eta)\n",
    "    # print(w_t[pop_masks_train[0]].sum(), w_t[pop_masks_train[1]].sum(), w_t[pop_masks_train[2]].sum(), w_t[pop_masks_train[3]].sum(), w_t[pop_masks_train[4]].sum())\n",
    "    w_all = np.concatenate([w_all, w_t.reshape(1, -1)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_P = np.concatenate([best_P, np.zeros((1, T - best_P.shape[1]))], axis=1)\n",
    "# P = best_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_all.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data and all models with weights as width of line\n",
    "# plt.figure(figsize=(5, 5))\n",
    "# plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], label='class 0', s=5)\n",
    "# plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], label='class 1', s=5)\n",
    "\n",
    "# for t in range(T):\n",
    "#     params = models[t]\n",
    "#     weight, bias = params['0.weight'].cpu().numpy(), params['0.bias'].cpu().numpy()\n",
    "#     w_t = P[0, t]\n",
    "#     x = np.linspace(-6, 6, 100)\n",
    "#     y = (-bias[0] - weight[0, 0] * x) / weight[0, 1]\n",
    "#     model.load_state_dict(params)\n",
    "#     print(model(torch.tensor([[-6., 6.]])).argmax())\n",
    "#     plt.plot(x, y, label='model {}'.format(t), linewidth=w_t * 10)\n",
    "# plt.legend()\n",
    "# plt.ylim(-6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision boundary of derandomized model\n",
    "def predict(x, P, models):\n",
    "    P = np.array(P).reshape(1, -1)\n",
    "    probs = []\n",
    "    for t in range(len(models)):\n",
    "        model.load_state_dict(models[t])\n",
    "        probs.append(model(x).detach().cpu().argmax(dim=1, keepdim=True))\n",
    "    probs = torch.cat(probs, dim=1)\n",
    "    preds = (probs @ P.T)[:, 0]\n",
    "    print(preds)\n",
    "    return preds > 0.5\n",
    "\n",
    "x_min, x_max = -6, 6\n",
    "y_min, y_max = -6, 6\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),\n",
    "                        np.arange(y_min, y_max, 0.05))\n",
    "pred = predict(torch.tensor(np.c_[xx.ravel(), yy.ravel()]).float(), [1] + [0] * (T - 1), models)\n",
    "# pred = predict(torch.tensor(np.c_[xx.ravel(), yy.ravel()]).float(), P, models)\n",
    "print(pred.shape, pred.sum())\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "# Put the result into a color plot\n",
    "# plt.figure(figsize=(5, 5))\n",
    "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], label='class 0', s=5)\n",
    "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], label='class 1', s=5)\n",
    "# plot the decision boundary but do not fill and add l\n",
    "sns.kdeplot(x=X_train[y_train == 0, 0], y=X_train[y_train == 0, 1], cmap=\"Blues\", shade=True, shade_lowest=False, alpha=0.7)\n",
    "sns.kdeplot(x=X_train[y_train == 1, 0], y=X_train[y_train == 1, 1], cmap=\"Oranges\", shade=True, alpha=0.3)\n",
    "plt.contour(xx, yy, pred.reshape(xx.shape), colors='tab:red', linewidths=0.3, alpha=0.7)\n",
    "# plt.contourf(xx, yy, pred.reshape(xx.shape), cmap=plt.cm.coolwarm, alpha=0.5)\n",
    "\n",
    "pred = predict(torch.tensor(np.c_[xx.ravel(), yy.ravel()]).float(), P, models)\n",
    "print(pred.shape, pred.sum())\n",
    "# only plot the decision boundary\n",
    "plt.contour(xx, yy, pred.reshape(xx.shape), colors='tab:green', linewidths=0.3, alpha=0.7)\n",
    "plt.legend()\n",
    "# in bold\n",
    "plt.annotate('RAI',(-4.5,-3), weight='bold', color='tab:green')\n",
    "plt.annotate('ERM',(-5,-1.3), weight='bold', color='tab:red')\n",
    "plt.xlim(-6, 6)\n",
    "plt.ylim(-4, 4)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "# plt.savefig('group_dro_rai.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(torch.tensor(np.c_[xx.ravel(), yy.ravel()]).float(), P, models)\n",
    "print(pred.shape, pred.sum())\n",
    "\n",
    "# Put the result into a color plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], label='class 0', s=5)\n",
    "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], label='class 1', s=5)\n",
    "plt.contourf(xx, yy, pred.reshape(xx.shape), cmap=plt.cm.coolwarm, alpha=0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_erm = np.zeros((1, T))\n",
    "P_erm[:, 0] = 1\n",
    "\n",
    "_, game_erm = raigame(P_erm, l_all, 0)\n",
    "_, game_opt = raigame(P, l_all, 0)\n",
    "\n",
    "print('game_erm = {}, game_opt = {}'.format(game_erm, game_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze CVaR Loss\n",
    "def get_cvar_loss(P, l, alpha):\n",
    "    exp_loss = (P @ l).reshape(-1)\n",
    "    exp_loss = np.sort(exp_loss)[::-1]\n",
    "    n = exp_loss.shape[0]\n",
    "    return format(exp_loss[:int(alpha * n)].mean(), '.3f')\n",
    "\n",
    "for alpha in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]:\n",
    "    print('alpha = {}, opt_cvar = {}, erm_cvar = {}'.format(alpha, get_cvar_loss(P, l_all, alpha), get_cvar_loss(P_erm, l_all, alpha)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze worst-class loss\n",
    "def get_worst_class_loss(P, l, dataset):\n",
    "    assert len(dataset) == l.shape[1]\n",
    "    exp_loss = (P @ l).reshape(-1)\n",
    "    num_classes = len(np.unique(dataset.y))\n",
    "    dct = {}\n",
    "    for i in range(num_classes):\n",
    "        idx = np.where(dataset.y == i)[0]\n",
    "        dct[i] = exp_loss[idx].mean()\n",
    "    print(\"Class Loss: \", dct)\n",
    "    print(\"Worst Class Loss: \", max(dct.values()))\n",
    "    return \n",
    "\n",
    "get_worst_class_loss(P, l_all, dataset_train)\n",
    "get_worst_class_loss(P_erm, l_all, dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
